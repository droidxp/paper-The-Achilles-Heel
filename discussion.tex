\section{Discussion}\label{sec:discussion}

In this section, we answer our research questions,
summarize the implications of our results, and discuss possible
limitations of our study that might threaten the
validity of the results presented so far.

\subsection{Answers to the Research Questions}

The results we presented in the previous sections
allow us to answer our three research questions, as
we summarize in the following.

\begin{itemize}
\item \textbf{Performance of the \mas on the \cds (RQ1).} 
  Our study indicates that the accuracy of the \mas reported in
  previous studies~\cite{DBLP:conf/wcre/BaoLL18,DBLP:journals/jss/CostaMMSSBNR22} does not
  generalize to a larger dataset. That is, while in our
  reproduction study (using the \sds of previous research) the \mas
  leads to an accuracy of \fscoreSmall, we observed a drop of precision and recall
  that leads to an accuracy of \fscore in the presence of our \cds (\apps pairs of
  original and repackaged versions of Android apps). 

%\item \textbf{Effect of trace analysis (RQ2).} We do not find any gain of enriching the vanilla \mas with trace analysis in terms of accuracy ($F_1$ score). Although the use of traces reduces the number of false negatives (improving recall), it also increases the number of false positives with a similar proportion---which does not change the $F_1$ score significantly. Nonetheless, for the context of malware identification, we believe that the gain in terms of recall might justify the use of trace analysis.

\item \textbf{Similarity Analysis (RQ2).} Our results bring evidence about the existence of an association between the similarity of the original and repackaged versions of an app and the ability of the \mas to correctly classify a repackaged version of an app as a malware. Therefore, the similarity assessment can explain the low performance of the \mas to identify malware in the \cds.

\item \textbf{Malware Family Analysis (RQ3).} The results indicate that some families are responsible for the largest number of false negatives in the complete dataset. We specifically further investigate the \gps and \rmb families, which corresponds to a particular type of Adwares, designed to automatically display advertisements while an app is running. After reverse engineering a sample of 60 malware apps from \gps and \rmb family, we confirmed that the \mas cannot identify the patterns of changes introduced in the repackaged versions of the apps. The prevalence of the \gps and \rmb in the \cds also contributed to explaining the poor performance of the \mas for malware classification in the large dataset.


\end{itemize}


\begin{comment} First, the proportion of malware samples in the
datasets differs. That is, \vt labels \review{$66.95$\%} of the repackaged version of the apps in the \cds as malware---contrasting with 67.64\% of the samples that \vt labels as malware in the \sds.
\end{comment}

\subsection{Implications}\label{sec:implications} 

Contrasting to previous research works~\cite{DBLP:conf/wcre/BaoLL18,DBLP:conf/iceccs/LeB0GL18,DBLP:journals/jss/CostaMMSSBNR22},
our results 
%discussed in the previous sections 
lead to a more systematic understanding
of the strengths and limitations of using the \mas
for malware classification. In particular, this is the
first study that empirically evaluates the \mas
considering as ground truth the outcomes
of \vt. This decision allowed us to explore the
\mas performance using well-known accuracy metrics (i.e., precision, recall, and
$F_1$ score). Previous studies assume that all repackaged versions of the
apps were malware. Our triangulation with \vt reveals this is not true. Although
the \mas presents a good accuracy for the \sds ($F_1$ = \fscoreSmall), 
in the presence of a large dataset the \mas accuracy drops significantly ($F_1$ = \fscore). 

\review{We also reveal that some families in the \cds are responsible for a large number of false negatives,
compromising the accuracy of the \mas.
Altogether, the takeaways of this research are twofold:}

\begin{itemize}
  \item Negative result: the \mas for malware detection exhibits a much higher false negative rate than previous research reported. 

  \item Future directions: Researchers should advance the \mas for malware detection by exploring more sophisticated techniques for differentiating between benign and malicious versions of apps. In particular, new approaches should leverage methods that can identify specific patterns of changes in repackaged versions and mine sensitive calls to native APIs. The versatility of the Java Native Interface (JNI) has introduced challenges, as malware authors are increasingly using the native layer to hide malicious code, making both static and dynamic analysis more challenging. The current state of the art in sandbox mining overlooks native calls.
  

    
\end{itemize}  


\subsection{Threats to Validity}\label{sec:threats}

% \todo[inline]{MM: Needs major revision.}

There are some threats to the validity of our results.
Regarding {\bf external validity}, one concern relates to the 
representativeness of our malware datasets and how generic our findings are.
Indeed, mitigating this threat was one of the motivations for our research,
since, in the existing literature, researchers had explored just
one dataset of 102 pairs of original/repackaged apps. Curiously,
for this small dataset, the performance of the
\mas is substantially superior
than its performance on our \cds (\apps pairs of
apps).

We contacted the authors of the Bao et al. research paper~\cite{DBLP:conf/wcre/BaoLL18}, asking them
if they had used any additional criteria for selecting the pairs of apps in their
dataset. Their answers suggest the contrary: they have not used
any particular app selection process that
could explain the superior performance of the \mas for the \sds. We believe that
our results in the \cds generalize better than previous research work,
since we have a more comprehensive collection of malware with different
families and degrees of similarity. Nonetheless, our
research focus only on Android repackaged malware and thus we cannot generalize
our findings to malware that targets other platforms or that use different approaches
to instantiate a malicious asset. Besides that, repackaging is a recurrent approach
for implementing Android malware.

Regarding {\bf conclusion validity}, during the exploratory phase of the \mas, we collected the set of calls to sensitive APIs the original version of
an app executes, while running a test case generation tool (DroidBot).
That is, the \mas assumes the existence of a benign original
version of a given app in the exploratory phase. \highlight{We also query \vt to confirm this
assumption, and found that the original version of seven (out 102) apps in the
\sds contains malicious code. We believe the authors of previous studies carefully check that assumption,
and this difference had occurred because the outputs of \vt change over time~\cite{DBLP:conf/uss/ZhuSYQZS020}}, and a dataset that is consistent at a date will
not so in the future.
Therefore, while reproducing this research, it is necessary to query \vt to get the most up-to-date classification of the assets, which might lead to results that might slightly
diverge from what we have reported here. Besides that, in the \cds we only consider
pairs of original/repackaged apps for which \vt classifies the original version as benign. 

\begin{comment}    
Regarding the \textbf{correlation between dataset properties and accuracy drop}, after running statistical tests (logistic regression),
we could not find evidence that the \emph{diversity} of the
complete dataset---in terms of similarity score and types of malware-
is responsible for the higher number of false negatives of the mining
sandbox approach. This implies that there was no 1-1 correlation between the brackets of similarity index, malware types to the drops in accuracy. Therefore, further research is necessary to investigate
other possible reasons for that. Perhaps, the complete dataset
contains a large percentage of malware that use more
advanced techniques to evade from both static and dynamic analysis---
both methods are used in the mining sandbox approach
we discussed in this paper.
\end{comment}


Regarding {\bf construct validity}, we address the main threats to our study by using simple and
well-defined metrics that are in use for this type of research: number of malware samples the
\mas correctly/wrongly classify in a dataset (true positives/false negatives).
Based on these metrics, we computed the accuracy results using precision and recall. In a preliminary study, we
investigated whether or not the \mas would classify an original version of an app as a malware,
computing the results of the test case generation tools in multiple runs. After combining three executions
in an original version to build a sandbox, we did not find any other execution that could wrongly
label an original app as a malware. Also, we label a repackaged version of an app as malware
  only if \vt reports that at least two engines detect suspicious behavior in that asset.
  This decision might be viewed as either a weak or strong constraint and could
  raise concerns about construct validity. However, when we relax this constraint and label an asset as malware whenever at least one engine detects suspicious behavior, precision improves to 0.85,
  but recall drops to 0.39. Overall, the accuracy of the \mas remains almost unchanged (F$_1$ = 0.53)---still significantly lower than the precision of the \mas for \sds.
  We also evaluated accuracy by considering an asset as malware when at least five or ten VirusTotal security engines flagged it. As shown in Table~\ref{tab:engines},
  the results did not diverge significantly.


\begin{table*}[htb]
  \caption{Accuracy of the \mas at \cds (4,076 pairs) based on engines.}
\centering{
  \begin{tabular}{lrrrrrr} \hline
    Engine(s) & TP   & FP  & FN  & Precision & Recall & $F_1$ \\
    \hline
    At least 01    & 1,222  & 220 & 1,900 & 0.85       & 0.39   & 0.53  \\
   
    At least 02    & 1,175  & 220 & 1,720 & 0.84       & 0.40   & 0.54  \\
   
    At least 05    & 1,087  & 220 & 1,578 & 0.83       & 0.40   & 0.54  \\
   
    At least 10    & 1,002  & 220 & 1,469 & 0.81       & 0.40   & 0.54  \\
    \hline
    
  \end{tabular}
  }
  \label{tab:engines}
\end{table*}

